# 🎥 YouTube Video Q&A App using Streamlit, LangChain & Groq (RAG-based)

This is a **Retrieval-Augmented Generation (RAG)** based Streamlit app that allows users to:
- Enter a **YouTube video ID**
- Automatically fetch the **transcript**
- Ask **natural language questions**
- Get accurate answers from an LLM based **only on the video content**
---

## 🚀 Features
- 🔎 Accepts **YouTube video IDs**  
- 🌐 Works with **manual and auto-generated captions**, any language  
- 🧠 Uses **HuggingFace embeddings** for semantic search  
- 🤖 Answers generated by **Groq LLaMA 3 (via LangChain)**  
- 🎛️ Clean two-page Streamlit interface  
- ✅ Uses only the transcript as context (no hallucinations)

---

## 🧰 Tech Stack

| Component                     | Description                                   |
|------------------------------|-----------------------------------------------|
| [Streamlit](https://streamlit.io/)        | UI and interaction                          |
| [LangChain](https://www.langchain.com/)   | RAG pipeline + LLM + prompt templating      |
| [Groq API](https://console.groq.com/)     | LLM provider (LLaMA3)                        |
| [YouTubeTranscriptAPI](https://pypi.org/project/youtube-transcript-api/) | Fetches subtitles from YouTube              |
| [FAISS](https://github.com/facebookresearch/faiss)       | Fast vector similarity search                |
| [HuggingFace Embeddings](https://huggingface.co/sentence-transformers) | Converts text into semantic vector space     |

---
## 🖥️ How It Works

1. User inputs a **YouTube video ID** (e.g., `rkKsMkOi3q4`)
2. The app fetches the **transcript** using `YouTubeTranscriptAPI`
3. Transcript is **chunked** into 1000-character blocks
4. Each chunk is embedded using `sentence-transformers/all-MiniLM-L6-v2`
5. Vector store is created with **FAISS**
6. User asks a question ➝ top-k similar transcript chunks are retrieved
7. These chunks + question are sent to **Groq’s LLaMA3 model** via LangChain
8. Answer is returned and displayed in the UI

---
